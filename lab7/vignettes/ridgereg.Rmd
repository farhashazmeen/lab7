---
title: <center>Ridgereg_caret Vignette </center>
subtitle: <center>farch587@student.liu.se, syesh076@student.liu.se</center>
author: <center>Farhana Chowdhury Tondra, Syeda Farha Shazmeen </center>
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::knitr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}{inputenc}
  
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA )
```

```{r}
library(mlbench)
library(caret)
library(leaps)
library(lab7)
data("BostonHousing")
```
caret package is used for classification and regression.

### Divide the BostonHousing data (or your own API data) into a test and training dataset using the caret package.

Dividing the BostonHousing data into 70% training and 30% test data 


```{r}
data("BostonHousing")
names(BostonHousing)
train_data <- caret::createDataPartition(BostonHousing$age, p = .7,
                                         list = FALSE,
                                         times= 1)
Trainingdata <- BostonHousing[train_data, ]
Testdata <- BostonHousing[-train_data, ]


nrow(Trainingdata)
nrow(Testdata)
```



###1. Fit a linear regression model on the training dataset.

The first two arguments of the train function are **predictor** and **outcome** data objects.Traincontrol object in train allows us to specify the resampling method.The resampling method used here is **cv-cross validation**. <br/>
For our analysis we have taken crim variable . R-squared  and RMSE(Root Mean Square Error)is used measure model performance. RSME values is low with  5.706505.
```{r}
ridge <- caret::train(crim~.,
                      data = Trainingdata,
                      method='lm',
                      trControl = trainControl(method = "cv")
)

print(ridge)
```


###2. Fit a linear regression model and with a linear regression model with forward selection of covariates.
Fitting a linear model with method = leapForward on the training dataset.
```{r}
lflmGrid <- expand.grid(nvmax=1:(ncol(Trainingdata)-1))
ridge <- caret::train(crim~.,
                      data = Trainingdata,
                      method='leapForward',
                      tuneGrid = lflmGrid
)
print(ridge)
```
###3. Evaluate the performance of this model on the training dataset.

since we have got a low RMSE,we think that our model has good perfomance with nvmax(number of predictors).

###4. Fit a ridge regression model using your ridgereg() function to the training dataset for different values of lambda.




```{r}
ridge <- ridgereg(crim ~ zn + indus + rad + medv,
                       data = Trainingdata ,
                       lambda = c(0.1,0.2)
)
ridge$print_QR()
```
###5. Find the best hyperparameter value for lambda using 10-fold cross-validation on the training set.
```{r echo=FALSE}
fitControl <- caret::trainControl(method = "cv",
                                    number = 10)
 lambdaGrid <- expand.grid(lambda = c(0,.01,.02,.03,.04))
  ridge <- caret::train(crim~.,
                        data = Trainingdata,
                        method='ridge',
                        trControl = fitControl,
                        tuneGrid = lambdaGrid,
                        preProcess=c('center', 'scale')
  )
  predict(ridge$finalModel, type='coef', mode='norm')$coefficients[13,]
  ridge.pred <- predict(ridge, Testdata)
  avgErrror<-2*sqrt(mean(ridge.pred - Testdata$crim)^2)
  print(ridge)

```

### 6.Evaluate the performance of all three models on the test dataset.

By evaluating three models of 

* Linear Regression
* Linear Regression with leapForward
* Ridge Regression

Ridge regression on training set with best value of lambda gives lower RMSE.

